[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quartotest",
    "section": "",
    "text": "Fixing jekyll on windows and macos devices.\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nNginx proxy manager\n\n\n\n\n\n\n\nnix\n\n\nlinux\n\n\ndevops\n\n\nnginx\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHow to get a subdomain from duckdns\n\n\n\n\n\n\n\nlinux\n\n\ndevops\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWhy schools should be less aggressive with content blocking\n\n\n\n\n\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nKasmweb setup\n\n\n\n\n\n\n\nnix\n\n\nlinux\n\n\ndevops\n\n\nubuntu\n\n\nkasm\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSetting up cockpit\n\n\n\n\n\n\n\naws\n\n\nec2\n\n\ndocker\n\n\nlinux\n\n\n\n\ncockpit is a gui to manage linux servers.\n\n\n\n\n\n\nSep 30, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-1-17-Jekyll on Windows and Mac.html",
    "href": "posts/2023-1-17-Jekyll on Windows and Mac.html",
    "title": "Fixing jekyll on windows and macos devices.",
    "section": "",
    "text": "sh <(curl -L https://nixos.org/nix/install) # After this, close and reopen your terminal. That is to reload your shell.\n\nnix-shell -p rubyPackages.ffi bundler\n\nbundle add execjs webrick wdm\n\nbundle install\n\nbundle exec jekyll serve --force_polling --livereload # if on windows\n\nbundle exec jekyll serve # if using macos or linux"
  },
  {
    "objectID": "posts/blocking/index.html",
    "href": "posts/blocking/index.html",
    "title": "Why schools should be less aggressive with content blocking",
    "section": "",
    "text": "I love experimenting with, and managing internet servers. Over the years, I have learned real world skills from my personal projects, as shown by the role I play my the computer science classes here at del norte. I do things, like create guides or fix broken one’s.\nBut when I try to access my server on school wifi, I get met with the below image.\n\nNot managed. My server is not blocked because it contains dangerous or harmful content, it’s blocked because my school’s (and so many others) uses a whitelist system. A whitelist means that everything is blocked by default, and things must explicitly be allowed through the firewall.\nNow I have a way around this. I can use a VPN, or similar software to get around the content filtering restrictions In addition to that, I can simply work on my home wifi, where there are no such restrictions. For me, getting access to my project based learning environments are easy.\nBut for students, who only use chromebooks, they cannot get around these restrictions. Chromebooks, the managed devices distributed by our school and so many others, are completely locked down. The chromebook itself implements the content blocking software, rather than the WiFi.\nThis means that, if a student were working at home on a school chromebook that was their only device, they would be denied resources that I have access to, purely because I have my own laptop. Information, research material, learning material, troubleshooting guides, and things like my digital playgrounds are locked away from those students.\nIt gets worse. Because the restrictions on the chromebooks prevent them from installing software, they cannot install the necessary tools needed for our computer science classes here at Del Norte. And unlike so many other computer based courses at Poway USD, the CompSci class does not provide it’s own computers for use. I personally know someone who wanted to take computer science, but was forced to drop out because they could not obtain their own device.\nObviously, this is unfair. The point of chromebooks is to offer students who are unable to obtain their own device for whatever reason the ability to have a computer to work on so they can be on equal footing with those who can afford their own devices. But what’s the purpose if they don’t actually offer students the ability to participate in the same activities and classes that students with more resources — wealth — have?\nChromebooks should be, at the very least, usable for all classes. But the only just thing is that students with only school provided chromebooks are given access to the exact same resources as those who can afford to buy their own device.\nThe reason why schools can and do provide chromebooks and other internet services is that they are cheap — Cheaper than they are normally. In America, there exists a program called e-rate, which is designed to make technology more accessible to public institutions, by providing discounts or potentially free products and services. The government subsidizes the cost.\nSadly, this program does not seem to cover computers (see section about internet access), which is probably why schools opt for chromeboks, the cheapest, lowest end device. Although it should be noted, that Google and Apple do have their own deals for offering discounts for bulk purchases for schools and other public institutions.\nAs great as e-rate is (it’s why the WiFi on school and college campus is so fast, if you know how to utilize it), it comes with caveats and conditions.\nThe same way the federal government holds funding for certain services over the head of state and local governments, the federal government holds the eligibility for e-rate over the head of schools and libraries. They must meet a certain requirements. One of those conditions is following CIPA, the Children’s Internet Protection Act.\nIn the beginning of this article, I linked an image of what our school’s block screen looks like. Quoting below:\n\nThe site you have requested has been blocked because it does not comply with the filtering requirements as described by the Children’s Internet Protection Act (CIPA).\n\nSo, when you click on CIPA, it takes you to the law itself. The law is very short, very vague, and the really important parts can fit comfortably into this article:\n\nSchools must implement a policy addressing these things. It does not say exactly what policies they must implement, or does it define materials harmful to minors, or even define “hacking”.\nBecause of the way the law is vaguely worded, schools are obligated to implement as aggressively as a content blocking policy as they can, including the complete prevention of installing software on chromebooks. Because the truth of the matter is, if someone can install VSCode (Programming application we use at Del Norte), then they can potentially install censorship circumvention software.\nIt’s very clear what is happening here. The school cannot meet both obiglations, one to prevent students from accessing resources, and another to make resources accessible to students. Becuase not attempting to block everything they can leads to loss on the e-rate discounts, Del Norte breaks it’s promise to it’s students to provide resources to them. Because to this institution, and many more, money is more important than the education of their students.\nNow, it’s not like I am personally not doing anything about this. In this blog, I have another post, about creating a system that will let students access. In addition to simply setting this system up, I am optimizing it, to make it cheaper and more accessible to students, especially those with less resources.\nOf course, as amazing as what I am deploying is, allowing students to get access to a fully featured linux desktop from their browser, it is also flawed — it also acts as a censorship circumvention software. Inside the system they have access to, users are given unfiltered content. Because of this, there is a possibility that schools would be obligated to block this as well.\nUltimately, my software is cool, equipping at least our computer science class the ability to be done on nothing but chromebooks, it is but a band-aid for the real problem — the laws that force schools further limit access to digital resources for students who already are lacking resources.\nIt does them little good. Off the top of my head, I know of several different ways to get around these content blocker, even on chromebooks. These blocking measures certainly do have an effect for general purpose use, but against a dedicated student, they are ultimately ineffective.\nFor some context, CIPA went into effect in 2001. The first iPhone came out in 2005. The law’s intentions are nice, but it pretty clearly wasn’t created with the foreknowledge that every student (who could afford one) would have a device in their pocket capable of circumventing pretty much all of the content blocking restrictions.\nBecause of this, despite the law being written to “protect” everybody, it only actually affects one group of people. Those who can only rely on school provided devices.\nThis is a form of institutional oppression of those who are in a lower socioeconomic class. Even though chromebooks are easily capable of installing the necessary software for computer science courses, laws in place force administrators to prevent students from doing so.\nThis is unjust. The restrictions on these laws should be lessened to enable students who already have a lesser access to such resources an equal access. If parents can be trusted to monitor their kid’s internet usage on a personal device, like a phone or macbook, why can’t they be trusted to do the same with a school chromebook?"
  },
  {
    "objectID": "posts/cockpit-setup/index.html",
    "href": "posts/cockpit-setup/index.html",
    "title": "Setting up cockpit",
    "section": "",
    "text": "What is cockpit (and similar softwares)\nAmazon lets us have free servers via EC2. The typical way to manage servers is either by sshing in, or using the cloud shell that Amazon (and Oracle) give. However, there are alternative ways to manage servers. One extremely popular example is pteradactyl, a webpage based gui to manage game (usually minecraft). It lets you download game servers as docker containers, run them, stop them, and maybe manage some basic settings, All the things a casual who just wants video games may need. But when I created a free Oracle server, I wanted something more. By this point, I was an experienced linux user, and I wanted more advanced features. So I searched for a more advanced server management tool, like people use on real servers, and I found cockpit.\nCockpit comes with many benefits. The two things I really like however, are that it’s terminal is not laggy at all, unlike the amazon ec2 cloud terminal, and it also offers a gui to manage docker containers.\n\n\nThe installation process\nThe installation process is simple:\nsudo apt install cockpit\nTo start the server, run:\nsudo systemctl enable --now cockpit\nThis sets the cockpit server to start on boot, and it starts it now.\nHowever, the firewall must open ports to allow the cockpit server through. This opens the default ports for the cockpit server. It should be noted that not every version fo linux uses ufw as a firewall, some use other firewalls with different management commands.\nsudo ufw allow 9090\nAnother important thing is to set the password for the default “ubuntu” user account so that you can login to cockpit.\nsudo passwd ubuntu\nIt will ask for the new password twice, not showing what you are typing.\nReboot the computer for the server to start properly, however, this won’t work as the virtual private cloud must have its ports open. I had to do this when I set up cockpit on my Oracle server, so I knew the gist of the steps.\n\n\nOpening EC2’s VPC ports (Also necessary if you want to host servers on ports other than 22, 80, or 443)\nFirst, go to your EC2 vps, where you would normally click connect from, and click on the link under vpc:\n\nThis should bring you up to a screen like this:\n\nClick the security tab, bringing you to a screen like this:\n\nAnd then click on security groups, bringing you to a screen like this:\n\nAnd then click on the “edit inbound rules”\nFinally, you should get something like this:\n\nAdd an item that matches what I have in the third row. That opens the port to allow cockpits server to escape. You may also need to use this page to open other ports if you are hosting servers on nonstandard ports."
  },
  {
    "objectID": "posts/duckdns/index.html",
    "href": "posts/duckdns/index.html",
    "title": "How to get a subdomain from duckdns",
    "section": "",
    "text": "Why?\nPeople can’t register for freenom consistently, and it can take time to get a domain from the one’s we have as a class. Duckdns allows people to create their own free domain, extremely easily, and nearly instantly.\n\n\nRegistration and Setup\n\nThis is the registration page. I really like duckdns because you only need a github account to login, which we already have.\n\nAfter you login, you will see this. You can get your own subdomain, and then set your ip address manually.\nTo find the ip address of your server, run this command on your AWS server:\ncurl ifconfig.me\nIf the command curl is not found, install it using apt.\nThen, you can manually input your server’s ip address into the duckdns website.\nIn your nginx confiuration file, make sure you set your nginx configuration file to be your domain name.\nserver {\n    listen 80;\n    listen [::]:80;\n    server_name [yoursubdomain].duckdns.org;\n\n    location / {\n        proxy_pass http://localhost:8087; # Make sure this matches the port your docker-compose.yml is set to\n        # Simple requests\n        if ($request_method ~* \"(GET|POST)\") {\n                add_header \"Access-Control-Allow-Origin\"  *;\n        }\n\n        # Preflight requests\n        if ($request_method = OPTIONS ) {\n                add_header \"Access-Control-Allow-Origin\"  *;\n                add_header \"Access-Control-Allow-Methods\" \"GET, POST, OPTIONS, HEAD\";\n                add_header \"Access-Control-Allow-Headers\" \"Authorization, Origin, X-Requested-With, Content-Type, Accept\";\n                return 200;\n        }\n    }\n}"
  },
  {
    "objectID": "posts/npm/index.html",
    "href": "posts/npm/index.html",
    "title": "Nginx proxy manager",
    "section": "",
    "text": "What is NPM and why do I want to use it?\nNginx proxy manager (npm, but not the node one) is a web based frontend for nginx that automatically also configures letsencrypt, similar to how certbot does it. It makes nginx much easier to use. Rather than writing config files, you can just click around, which is much easier. For a high school computer science class, I think NPM is better, because it doesn’t have the complexity of npm (less potential for accidental failurs), but still teaches people about ports mapping, encryption, and other necessary skills.\nIn addition to that, with npm, even if someone does create a bad config, only their server goes down. With npm, a bad nginx config leads to the whole server going down. That is… not optimal.\n\n\nInstallation and Setup\nInstallation is simpleish.\nFirst, create a docker network for usage with our docker containers (step from here). Because these are our school projects, I will call that network nighthawks\ndocker network create nighthawks\nCreate a folder called npm, and put a docker-compose.yml in it (basic compose file from here):\nversion: '3'\nservices:\n  app:\n    image: 'jc21/nginx-proxy-manager:latest'\n    restart: unless-stopped\n    ports:\n      - '80:80'\n      - '81:81'\n      - '443:443'\n    volumes:\n      - ./data:/data\n      - ./letsencrypt:/etc/letsencrypt\n\nnetworks:\n  default:\n    external: true\n    name: nighthawks\n\ndocker-compose up -d and you’re good to go. It should be noted that you need to have ports 443 and 80 unused by anything else, like Nginx proper. So if you are running nginx, stop it first before you up NPM.\nNPM does need to have port 81 accessible.\nYou can either use a reverse proxy, or open up the port to be accessible from the internet.\nIf you want the port to be accessible from the internet, you might have a firewall of some kind, so just open that. And if you are using one of the big cloud providers (aws, azure, oracle), then you also might have to configure security control groups, as that acts as an extra firewall for those server types. See my cockpit guide on how to do this with AWS.\nIf you want to do a reverse proxy, just use npm to do it: Use proxy post to connect http://npm_app_1:81 to a domain name.\nNow, to configure npm, just access the web interface at https://[domainname/ip]:81\n\n\nUsage\nYou may notice above, in the section about using a reverse to expose npm, I use the docker container name, rather than a port. That’s the amazing part of npm. As long as your docker containers are on the same network, all you need is a hostname and the used port. You don’t even need to expose ports in your docker-compose.yml\nSo rather than the docker-compose.yml we use in our deployment guide\nWe can use:\n\n\n\nversion: '3'\nservices:\n      web:\n              image: flask_port_v1\n              build: .\n              #ports: # ports section not needed\n                     # - \"8086:8080\"\n              volumes:\n                      - persistent_volume:/app/volumes\nvolumes:\npersistent_volume:\n  driver: local\n  driver_opts:\n    o: bind\n    type: none\n    device: /home/ubuntu/flask_portfolio/volumes\n    # replace just flask_portfolio\n\nnetworks:\n  default:\n    external: true\n    name: nighthawks\n\nAnd then you can simply expose http://flask_port_v1_web_1:8080 to the world!"
  },
  {
    "objectID": "posts/setting-up-kasm/index.html",
    "href": "posts/setting-up-kasm/index.html",
    "title": "Kasmweb setup",
    "section": "",
    "text": "School chromebooks cannot be used for computer science. Due to a content blocking setup that is more restrictive than the Great Firewall of China, school chromebooks cannot install the necessary digital tools for software development.\nCurrently, you must have your own device to be able to participate in computer science classes. Students who are unable to obtain their own device for whatever reason are denied participation.\nAlthough getting the chromebooks unlocked to be able to install software is a complex, potentially legal problem, there are alternatives.\nKasm is a remote desktop software. It runs a computer on a remote server, that can be accessed through a browser, or a chromebook. Because there are no restrictions on what can be installed when using Kasm, it makes it possible to use development tools on a chromebook. This blog post is me optimizing kasm so that it is more resource efficient, and also enabling software development tools to be easily installed on it.\nRoadblocks/steps:"
  },
  {
    "objectID": "posts/setting-up-kasm/index.html#turns-out-memory-deduplication-is-on-by-default-for-docker-containers",
    "href": "posts/setting-up-kasm/index.html#turns-out-memory-deduplication-is-on-by-default-for-docker-containers",
    "title": "Kasmweb setup",
    "section": "Turns out, memory deduplication is on by default for docker containers",
    "text": "Turns out, memory deduplication is on by default for docker containers\nGithub issue where someone asked about this. The documents linked were very unclear, so I’ll break it down.\nIf you are using overlayfs or aufs, you have memory deduplication. If you are using other storage drivers, you sacrifice memory for more i/o (write/read) performance.\nFrom here:\n\nOn my ubuntu virtual machine, and the AWS ubuntu machine we are working on, Overlay2 is the storage driver:"
  },
  {
    "objectID": "posts/setting-up-kasm/index.html#kernel-same-page-merging",
    "href": "posts/setting-up-kasm/index.html#kernel-same-page-merging",
    "title": "Kasmweb setup",
    "section": "Kernel Same page merging",
    "text": "Kernel Same page merging\nPreviously, I tried instructions from here: https://wiki.openvz.org/KSM_(kernel_same-page_merging). However, I noticed only a minimal space saved using the LD_PRELOAD steps. Not useful.\nI then tried cachyos fork of uksmd: https://github.com/CachyOS/uksmd, a daemon to go through userspace tasks and dedupe them.\nOnly works with a kernel that has the pmadv_ksm() syscall. Exists in most kernels optimized for desktop usage, like linux-zen, linux-liqourix, or pf-kernel (the original creators of uksmd)\nTo check if your currently running kernel has the feature:\n\non Archlinux, check if the files sys_enter_pmadv_ksm and sys_exit_pmadv_ksm exist in /sys/kernel/debug/tracing/events/syscalls (default does not have this feature, but linux-zen does)\non Ubuntu check if lines containing pmadv exist in the file /proc/kallsyms\n\n\n\n\nuksmstats\n\n\nHalf a gig of ram saved on a normal desktop. Expect to see much more when multiple almost identical docker containers are launched. Very useful. It saves a lot of ram. However, there might be a better way for docker, without jumping through hoops.\nDoes cost a miniscule amount of cpu power, but we have more cpu power and less ram on our servers.\nTo install uksmd on ubuntu, you need to switch kernels.\n\nCompiling UKSMD\nSteps to do so on Ubuntu 22 (you must have switched kernels):\nsudo apt-get install debhelper build-essential dh-make meson pkg-config libprocps-dev libcap-ng-dev # I think it can either be pkg-conf or pkg-config\ngit clone https://github.com/insilications/uksmd-clr\nRename the directory to be something compatible with below steps, like uksmd-1 before you cd into it.\nFollow steps from here\ndh_make --createorig\ndh_auto_configure --buildsystem=meson\ndpkg-buildpackage -rfakeroot -us -uc -b\nThe debian package will be build in the directory above the source directory.\nInstall your debian package!\nIf you want the uksmdstats command for monitoring purposes, you can only get it from the cachyos github (or make your own, it’s just a shell script).\nsudo curl https://raw.githubusercontent.com/CachyOS/uksmd/master/uksmdstats -o /usr/bin/uksmdstats\n\n\nSwitching Kernels\nstatus: complete\ncurl 'https://liquorix.net/install-liquorix.sh' | sudo bash from the liqourix kernel website\nI checked if liqourix has the necessary features, and yes it does.\n\nSetting the Default Kernel\nstatus: researching\nReddit post where I ask how to set default kernel in grub\nOn that reddit post, I talk about some flawed solutions I have found. Mainly they don’t seem to be truly persistent, not surviving kernel updates, updates of grub, or installation of new kernels. This endeavor is pretty risky, because a broken grub means we will have no choice but to delete our aws and start over. I need a 100% solution.\nAfter grilling chatgpt through 3 wrong answers, which chatgpt presented with the absolute confidence that an AI has, it finally presented me a solution that seems like it doesn’t have a risk of breaking the AWS system we are working on.\n\nI need to make some adjustments, but I should be able to select for the term “liqourix” while removing the term “recovery” to select the correct kernel (but not the recovery kernel) with complete consistency even through kernel updates, grub updates, or installation of new kernels.\nI searched around for how to do this, but I eventually gave up and asked chatgpt again, getting this:\n\nMy 20_linux_xen is not the same as what chatgpt wants, so I asked again, and it gave me this code to put in /etc/default/grub:\n# Set the default menu entry based on the title of the menu entry\n# that contains the word \"liqourix\" while ignoring the term \"recovery\"\nGRUB_DEFAULT=\"$(grep -E -o '^menuentry.*liqourix.*' /boot/grub/grub.cfg | grep -v 'recovery' | head -1 | awk -F\\' '{print $2}')\"\ngrep -E -o '^menuentry.*liqourix.*' /boot/grub/grub.cfg | grep -v 'recovery' | head -1 | awk -F\\' '{print $2}'\nI will test this in a virtual machine.\n\n\n\nWeaker alternative: ksm_preload\nstatus: won’t be used\ngit clone https://github.com/binfess/ksm_preload\ncmake .\nmake\nsudo make install\nI added LD_PRELOAD=/usr/local/share/ksm_preload/libksm_preload.so to the file /etc/environment\nI haven’t tested the above, but I saw very minimal space saved, only about 0.11 **megabytes* saved, on my desktop. Tests on my sample server are similarly discouraging:\n\nThe above was with 2 kasm sessions open. Nearly useless. In addition to that, uksmd seems to make this completely obsolete."
  },
  {
    "objectID": "posts/setting-up-kasm/index.html#zram",
    "href": "posts/setting-up-kasm/index.html#zram",
    "title": "Kasmweb setup",
    "section": "zram",
    "text": "zram\nstatus: immplemented on my personal systems, but not on the ubuntu vm yet.\nTo install zram, sudo apt install systemd-zram-generator\nThen, you can configure zram by editing /etc/systemd/zram-generator.conf\nThis works, but apparently, things in a swap file aren’t deduplicated by uksmd. Rather, zram handles it’s own deduplication, with the compression algorithms. Becuase it must hash pages, this can potentially lead to more cpu usage.\nBecause I don’t know about this, I made yet another reddit post asking about this topic."
  },
  {
    "objectID": "posts/setting-up-kasm/index.html#zswap",
    "href": "posts/setting-up-kasm/index.html#zswap",
    "title": "Kasmweb setup",
    "section": "zswap",
    "text": "zswap\nStatus: Partially done, but dropped in favor of zswap\n\nzswap is disabled by default on my ubuntu virtual machine. Odd that both are disabled by default.\nParameters for zswap can be found in /sys/module/zswap/parameters/\nTo set parameters at boot, use kernel boot paremeters, like zswap.enabled=1 zswap.compressor=lz4 zswap.max_pool_percent=20 zswap.zpool=z3fold\nI will need to tinker to see what is the most optimized zswap setup\nzswap has it’s own memory deduplication feature, which is enabled by default on both my ubuntu vm and the aws ubuntu server:\nSee above, my comments about zswap’s memory deduplication feature."
  }
]